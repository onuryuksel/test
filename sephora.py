import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import json
from io import StringIO
import time # Gerekirse gecikme eklemek iÃ§in

# Sayfa BaÅŸlÄ±ÄŸÄ±
st.set_page_config(page_title="Sephora Marka Filtre Ã‡ekici", layout="wide")
st.title("ğŸ’„ Sephora Marka Filtre Veri Ã‡ekici")
st.caption("Sephora Ã¼rÃ¼n listeleme sayfasÄ± linkini girerek marka filtresindeki verileri CSV olarak indirin.")

# --- Fonksiyonlar ---

@st.cache_resource # Session objesini cache'le
def get_session():
    """Tek bir requests.Session nesnesi oluÅŸturur."""
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,tr-TR;q=0.8,tr;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Sec-Ch-Ua': '"Not.A/Brand";v="8", "Chromium";v="114", "Google Chrome";v="114"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"',
        'Cache-Control': 'max-age=0',
    }
    session.headers.update(headers)
    return session

def fetch_html_requests(url):
    """Verilen URL'den HTML iÃ§eriÄŸini requests.Session ile Ã§eker (Timeout artÄ±rÄ±ldÄ±)."""
    session = get_session()
    # Timeout sÃ¼resini 60 saniyeye Ã§Ä±karalÄ±m
    timeout_seconds = 60
    try:
        # time.sleep(0.5)
        st.info(f"URL alÄ±nÄ±yor (Timeout={timeout_seconds}s)... LÃ¼tfen bekleyin.")
        response = session.get(url, timeout=timeout_seconds, allow_redirects=True)
        response.raise_for_status()

        if not response.content:
             st.warning(f"URL'den boÅŸ iÃ§erik alÄ±ndÄ±: {url}. Sayfa yapÄ±sÄ± veya engelleme sorunu olabilir.")
             return None

        response.encoding = response.apparent_encoding or 'utf-8'
        st.success(f"URL baÅŸarÄ±yla alÄ±ndÄ± (requests): {url}")
        return response.text

    except requests.exceptions.Timeout:
        st.error(f"Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ± (Timeout={timeout_seconds}s): {url}")
        st.info("Sunucu yanÄ±t vermedi. Sunucu yoÄŸun olabilir veya istek engelleniyor olabilir.")
        return None
    except requests.exceptions.HTTPError as e:
         st.error(f"HTTP HatasÄ±: {e.response.status_code} {e.response.reason} - URL: {url}")
         if e.response.status_code == 403:
              st.warning("403 Forbidden hatasÄ± alÄ±ndÄ±. Bu genellikle bot korumasÄ± anlamÄ±na gelir.")
         elif e.response.status_code >= 500:
              st.warning("Sunucu hatasÄ± alÄ±ndÄ±. Site geÃ§ici olarak eriÅŸilemez olabilir.")
         return None
    except requests.exceptions.ConnectionError as e:
        st.error(f"BaÄŸlantÄ± HatasÄ±: {e}")
        st.info("BaÄŸlantÄ± kurulamadÄ± veya sunucu baÄŸlantÄ±yÄ± kapattÄ±. URL'yi kontrol edin veya aÄŸ sorunlarÄ±nÄ±/engellemeleri deÄŸerlendirin.")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"Genel URL alÄ±nÄ±rken hata oluÅŸtu: {e}")
        return None

# --- Marka Ã‡Ä±karma FonksiyonlarÄ± (Ã–nceki kodla aynÄ±) ---

def find_brand_filter(data):
    """Recursive olarak JSON/dictionary iÃ§inde 'attributeId': 'c_brand' arar."""
    if isinstance(data, dict):
        if data.get('attributeId') == 'c_brand' and 'values' in data:
            if data['values'] and isinstance(data['values'], list) and all(isinstance(item, dict) for item in data['values']):
                 return data
        for key, value in data.items():
            if key not in ['image', 'images', 'icon', 'icons', 'banner', 'banners', 'variations', 'attributes']: # BÃ¼yÃ¼k listeleri/dict'leri atla
                result = find_brand_filter(value)
                if result:
                    return result
    elif isinstance(data, list):
        # Listeyi kÃ¼Ã§Ã¼ltme (Ã§ok uzun listelerde performansÄ± artÄ±rabilir - riskli)
        # items_to_check = data[:500] if len(data) > 500 else data
        items_to_check = data
        for item in items_to_check:
            result = find_brand_filter(item)
            if result:
                return result
    return None

def extract_brands_directly(soup):
    """Alternatif yÃ¶ntem: DoÄŸrudan HTML elementlerini parse etmeye Ã§alÄ±ÅŸÄ±r."""
    st.info("Alternatif yÃ¶ntem: HTML elementleri taranÄ±yor...")
    brands_data = []
    try:
        # 'Brands' baÅŸlÄ±ÄŸÄ±nÄ± veya benzerini bul
        brand_header = soup.find(['h3', 'h2', 'div', 'button'], string=re.compile(r'Brands?', re.IGNORECASE))

        # Filtre container'Ä±nÄ± bul
        brand_section = None
        if brand_header:
            # BaÅŸlÄ±ktan birkaÃ§ seviye yukarÄ± Ã§Ä±kmayÄ± dene
            current = brand_header
            for _ in range(5): # En fazla 5 seviye yukarÄ± bak
                 parent = current.find_parent(['div', 'section', 'aside'])
                 if parent and parent.find(['input', 'label'], class_=re.compile(r'checkbox|facet', re.IGNORECASE)):
                      brand_section = parent
                      st.info(f"BaÅŸlÄ±ktan parent bulundu: <{brand_section.name} class='{brand_section.get('class', 'N/A')}'>")
                      break
                 if not parent: break # Daha fazla parent yoksa dur
                 current = parent

        # EÄŸer baÅŸlÄ±ktan bulunamazsa, genel filtre container'larÄ±nÄ± ara
        if not brand_section:
            st.warning("Marka baÅŸlÄ±ÄŸÄ±ndan filtre bÃ¶lÃ¼mÃ¼ bulunamadÄ±. Genel arama...")
            # Checkbox iÃ§eren veya Ã§ok sayÄ±da filtre deÄŸeri iÃ§eren div'leri ara
            possible_sections = soup.find_all('div', class_=re.compile(r'filter|facet|refine', re.IGNORECASE))
            for section in possible_sections:
                # Ä°Ã§inde 'brand' geÃ§en bir input veya Ã§ok sayÄ±da checkbox/label var mÄ±?
                 if section.find('input', attrs={'name': re.compile(r'brand', re.IGNORECASE)}) or \
                    len(section.find_all(['label', 'li', 'div'], class_=re.compile(r'checkbox|facet-value', re.IGNORECASE))) > 3: # EÅŸik deÄŸeri
                    brand_section = section
                    st.info(f"Genel aramada olasÄ± filtre bÃ¶lÃ¼mÃ¼ bulundu: <{brand_section.name} class='{brand_section.get('class', 'N/A')}'>")
                    break

        if not brand_section:
            st.error("Marka filtresi bÃ¶lÃ¼mÃ¼ HTML iÃ§inde bulunamadÄ± (Alternatif YÃ¶ntem).")
            return None

        # Marka item'larÄ±nÄ± bul (label veya input iÃ§eren div/li)
        brand_items = brand_section.find_all(['div', 'li', 'label'], class_=re.compile(r'checkbox|facet-value|filter-value|facet-label', re.IGNORECASE))

        if not brand_items:
            # Sadece input'larÄ± bulup parent'larÄ±ndan text almayÄ± dene
            inputs = brand_section.find_all('input', type='checkbox', attrs={'name': re.compile(r'brand', re.IGNORECASE)})
            brand_items = [inp.parent for inp in inputs if inp.parent] # Parent'larÄ± al

        if not brand_items:
            st.error("Marka listesi elementleri (item/label) bulunamadÄ±.")
            return None

        st.info(f"Bulunan olasÄ± marka elementi sayÄ±sÄ±: {len(brand_items)}")

        extracted_brands = set() # Marka isimlerinin tekrarÄ±nÄ± Ã¶nlemek iÃ§in
        for item in brand_items:
            brand_name = ""
            count = 0
            text_content = item.get_text(separator=' ', strip=True)

            # Regex: Marka AdÄ± (SayÄ±) veya Marka AdÄ± SayÄ±
            match = re.search(r'^(.*?)\s*\(?(\d+)\)?$', text_content)
            if match:
                brand_name = match.group(1).strip()
                count = int(match.group(2))
                # AnlamsÄ±z isimleri filtrele
                if brand_name.lower() not in ['no', 'yes', ''] and brand_name not in extracted_brands:
                    brands_data.append({'Marka': brand_name,'ÃœrÃ¼n SayÄ±sÄ±': count})
                    extracted_brands.add(brand_name)

        if brands_data:
            st.success(f"{len(brands_data)} marka verisi doÄŸrudan HTML'den baÅŸarÄ±yla Ã§ekildi (Alternatif YÃ¶ntem).")
            return pd.DataFrame(brands_data) # Zaten unique olmalÄ± ama emin olmak iÃ§in drop_duplicates kullanÄ±labilir
        else:
            st.warning("DoÄŸrudan HTML taramasÄ±nda yapÄ±sal marka verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
            return None

    except Exception as e:
        st.error(f"MarkalarÄ± doÄŸrudan HTML'den Ã§ekerken hata oluÅŸtu (Alternatif YÃ¶ntem): {e}")
        return None


def extract_brands_from_html(html_content):
    """HTML iÃ§eriÄŸinden marka verilerini Ã§Ä±karÄ±r (__NEXT_DATA__ Ã¶ncelikli)."""
    if not html_content:
        st.error("HTML iÃ§eriÄŸi alÄ±namadÄ±.")
        return None

    soup = BeautifulSoup(html_content, 'lxml')
    brands_data = []

    script_tag = soup.find('script', id='__NEXT_DATA__')
    if script_tag:
        st.info("__NEXT_DATA__ script'i bulundu, iÅŸleniyor...")
        try:
            next_data = json.loads(script_tag.string)
            brand_filter = find_brand_filter(next_data)

            if brand_filter and 'values' in brand_filter and isinstance(brand_filter['values'], list):
                processed_brands = set()
                for item in brand_filter['values']:
                    if isinstance(item, dict) and 'label' in item and 'hitCount' in item and item['label'] and item['hitCount'] is not None:
                        brand_name = item['label'].strip()
                        if brand_name.lower() != 'no' and brand_name not in processed_brands:
                             brands_data.append({'Marka': brand_name,'ÃœrÃ¼n SayÄ±sÄ±': item['hitCount']})
                             processed_brands.add(brand_name)
                if brands_data:
                     st.success(f"{len(brands_data)} marka verisi __NEXT_DATA__ iÃ§inden baÅŸarÄ±yla Ã§ekildi.")
                     return pd.DataFrame(brands_data)
                else:
                     st.warning("__NEXT_DATA__ iÃ§inde marka verisi bulundu ancak iÅŸlenebilir formatta deÄŸildi veya sadece 'No' etiketleri vardÄ±. Alternatif yÃ¶ntem deneniyor...")
            else:
                st.warning("__NEXT_DATA__ iÃ§inde 'c_brand' attributeId'li veya 'values' listesi iÃ§eren geÃ§erli yapÄ± bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")
        except Exception as e:
            st.error(f"__NEXT_DATA__ iÅŸlenirken hata: {e}. Alternatif yÃ¶ntem deneniyor...")
    else:
        st.warning("__NEXT_DATA__ script'i bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")

    # Alternatif YÃ¶ntem
    return extract_brands_directly(soup)


# --- Streamlit ArayÃ¼zÃ¼ ---
url = st.text_input("Sephora ÃœrÃ¼n Listeleme SayfasÄ± URL'sini Girin:", placeholder="https://www.sephora.me/ae-en/shop/fragrance/C301")
process_button = st.button("MarkalarÄ± Ã‡ek ve CSV OluÅŸtur")

if process_button and url:
    if not url.startswith(('http://', 'https://')) or "sephora." not in url:
         st.warning("LÃ¼tfen geÃ§erli bir Sephora URL'si girin (http:// veya https:// ile baÅŸlamalÄ±dÄ±r).")
    else:
        with st.spinner("Sayfa indiriliyor (requests) ve markalar Ã§ekiliyor... LÃ¼tfen bekleyin."):
            # Yeniden requests ile dene (uzun timeout ile)
            html_content = fetch_html_requests(url)

            if html_content:
                df_brands = extract_brands_from_html(html_content)

                if df_brands is not None and not df_brands.empty:
                    st.subheader("Ã‡ekilen Marka Verileri")
                    st.dataframe(df_brands, use_container_width=True)

                    try:
                        csv_buffer = StringIO()
                        df_brands.to_csv(csv_buffer, index=False, encoding='utf-8')
                        csv_data = csv_buffer.getvalue()
                        try:
                            path_part = url.split('/')[-1].split('?')[0]
                            safe_part = re.sub(r'[\\/*?:"<>|]', "-", path_part)
                            safe_part = safe_part[:50] if len(safe_part) > 50 else safe_part
                            csv_filename = f"sephora_markalar_{safe_part}.csv" if safe_part else "sephora_markalar.csv"
                        except Exception:
                            csv_filename = "sephora_markalar.csv"

                        st.download_button(
                            label="ğŸ’¾ CSV Olarak Ä°ndir",
                            data=csv_data,
                            file_name=csv_filename,
                            mime='text/csv',
                        )
                    except Exception as e:
                        st.error(f"CSV dosyasÄ± oluÅŸturulurken veya indirme butonu hazÄ±rlanÄ±rken hata: {e}")

                elif df_brands is not None and df_brands.empty:
                     st.warning("Marka verisi bulunamadÄ±. URL'yi, filtrelerin gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ kontrol edin veya sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir.")
            # else: fetch_html_requests iÃ§inde hata mesajÄ± zaten verildi.

elif process_button and not url:
    st.warning("LÃ¼tfen bir URL girin.")

st.markdown("---")
st.caption("Not: Bu uygulama Sephora web sitesinin yapÄ±sÄ±na baÄŸlÄ±dÄ±r. Site gÃ¼ncellenirse veya gÃ¼venlik Ã¶nlemleri (WAF vb.) kullanÄ±lÄ±rsa `requests` ile veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z olabilir.")
