import streamlit as st
import requests # Hata yakalama vb. iÃ§in kalabilir
import cloudscraper # Eklendi
from bs4 import BeautifulSoup
import pandas as pd
import re
import json
from io import StringIO
import time

# Sayfa BaÅŸlÄ±ÄŸÄ±
st.set_page_config(page_title="Sephora Marka Filtre Ã‡ekici", layout="wide")
st.title("ğŸ’„ Sephora Marka Filtre Veri Ã‡ekici")
st.caption("Sephora Ã¼rÃ¼n listeleme sayfasÄ± linkini girerek marka filtresindeki verileri CSV olarak indirin.")

# --- Fonksiyonlar ---

@st.cache_resource
def get_scraper():
    """Cloudscraper Ã¶rneÄŸi oluÅŸturur."""
    scraper = cloudscraper.create_scraper(
        browser={ # TarayÄ±cÄ± gibi gÃ¶rÃ¼nmek iÃ§in ayarlar
            'browser': 'chrome',
            'platform': 'windows',
            'mobile': False,
            'desktop': True,
        },
         delay=10 # Cloudflare kontrolleri iÃ§in gecikme (isteÄŸe baÄŸlÄ±)
    )
    # Gerekirse ek header'lar eklenebilir, cloudscraper zaten yÃ¶netir ama deneyebiliriz.
    scraper.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,tr-TR;q=0.8,tr;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    })
    return scraper

def fetch_html_cloudscraper(url):
    """Verilen URL'den HTML iÃ§eriÄŸini cloudscraper ile Ã§eker."""
    scraper = get_scraper()
    timeout_seconds = 60 # Timeout sÃ¼resini yÃ¼ksek tutalÄ±m
    try:
        st.info(f"URL alÄ±nÄ±yor (cloudscraper, Timeout={timeout_seconds}s)... LÃ¼tfen bekleyin.")
        response = scraper.get(url, timeout=timeout_seconds)
        response.raise_for_status()

        if not response.content:
             st.warning(f"URL'den boÅŸ iÃ§erik alÄ±ndÄ±: {url}.")
             return None

        # Cloudscraper genellikle decode iÅŸlemini yapar ama kontrol edelim
        html_content = response.text
        st.success(f"URL baÅŸarÄ±yla alÄ±ndÄ± (cloudscraper): {url}")
        return html_content

    except requests.exceptions.Timeout: # cloudscraper da requests tabanlÄ±
        st.error(f"Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ± (Timeout={timeout_seconds}s): {url}")
        st.info("Sunucu yanÄ±t vermedi. Sunucu yoÄŸun olabilir veya cloudscraper engeli aÅŸamadÄ±.")
        return None
    except requests.exceptions.HTTPError as e:
         st.error(f"HTTP HatasÄ±: {e.response.status_code} {e.response.reason} - URL: {url}")
         if e.response.status_code == 403:
              st.warning("403 Forbidden hatasÄ± alÄ±ndÄ±. Cloudflare/WAF korumasÄ± aktif olabilir ve aÅŸÄ±lamamÄ±ÅŸ olabilir.")
         elif e.response.status_code >= 500:
              st.warning("Sunucu hatasÄ± alÄ±ndÄ±.")
         return None
    except requests.exceptions.ConnectionError as e:
        st.error(f"BaÄŸlantÄ± HatasÄ±: {e}")
        st.info("BaÄŸlantÄ± kurulamadÄ± veya sunucu baÄŸlantÄ±yÄ± kapattÄ±. URL'yi kontrol edin veya aÄŸ sorunlarÄ±nÄ±/engellemeleri deÄŸerlendirin.")
        return None
    except Exception as e: # cloudscraper'Ä±n kendi hatalarÄ± olabilir
        st.error(f"Cloudscraper ile URL alÄ±nÄ±rken hata oluÅŸtu: {e}")
        return None


# --- Marka Ã‡Ä±karma FonksiyonlarÄ± (Ã–nceki kodla aynÄ±) ---

def find_brand_filter(data):
    """Recursive olarak JSON/dictionary iÃ§inde 'attributeId': 'c_brand' arar."""
    if isinstance(data, dict):
        if data.get('attributeId') == 'c_brand' and 'values' in data:
            if data['values'] and isinstance(data['values'], list) and all(isinstance(item, dict) for item in data['values']):
                 return data
        for key, value in data.items():
            if key not in ['image', 'images', 'icon', 'icons', 'banner', 'banners', 'variations', 'attributes']:
                result = find_brand_filter(value)
                if result:
                    return result
    elif isinstance(data, list):
        items_to_check = data
        for item in items_to_check:
            result = find_brand_filter(item)
            if result:
                return result
    return None

def extract_brands_directly(soup):
    """Alternatif yÃ¶ntem: DoÄŸrudan HTML elementlerini parse etmeye Ã§alÄ±ÅŸÄ±r."""
    st.info("Alternatif yÃ¶ntem: HTML elementleri taranÄ±yor...")
    # ... (Bu fonksiyon Ã¶ncekiyle aynÄ±, detaylÄ± arama yapÄ±yor) ...
    brands_data = []
    try:
        brand_header = soup.find(['h3', 'h2', 'div', 'button'], string=re.compile(r'Brands?', re.IGNORECASE))
        brand_section = None
        if brand_header:
            current = brand_header
            for _ in range(5):
                 parent = current.find_parent(['div', 'section', 'aside'])
                 if parent and parent.find(['input', 'label'], class_=re.compile(r'checkbox|facet', re.IGNORECASE)):
                      brand_section = parent
                      st.info(f"BaÅŸlÄ±ktan parent bulundu: <{brand_section.name} class='{brand_section.get('class', 'N/A')}'>")
                      break
                 if not parent: break
                 current = parent
        if not brand_section:
            st.warning("Marka baÅŸlÄ±ÄŸÄ±ndan filtre bÃ¶lÃ¼mÃ¼ bulunamadÄ±. Genel arama...")
            possible_sections = soup.find_all('div', class_=re.compile(r'filter|facet|refine', re.IGNORECASE))
            for section in possible_sections:
                 if section.find('input', attrs={'name': re.compile(r'brand', re.IGNORECASE)}) or \
                    len(section.find_all(['label', 'li', 'div'], class_=re.compile(r'checkbox|facet-value', re.IGNORECASE))) > 3:
                    brand_section = section
                    st.info(f"Genel aramada olasÄ± filtre bÃ¶lÃ¼mÃ¼ bulundu: <{brand_section.name} class='{brand_section.get('class', 'N/A')}'>")
                    break
        if not brand_section:
            st.error("Marka filtresi bÃ¶lÃ¼mÃ¼ HTML iÃ§inde bulunamadÄ± (Alternatif YÃ¶ntem).")
            return None

        brand_items = brand_section.find_all(['div', 'li', 'label'], class_=re.compile(r'checkbox|facet-value|filter-value|facet-label', re.IGNORECASE))
        if not brand_items:
             inputs = brand_section.find_all('input', type='checkbox', attrs={'name': re.compile(r'brand', re.IGNORECASE)})
             brand_items = [inp.parent for inp in inputs if inp.parent]

        if not brand_items:
            st.error("Marka listesi elementleri (item/label) bulunamadÄ±.")
            return None

        st.info(f"Bulunan olasÄ± marka elementi sayÄ±sÄ±: {len(brand_items)}")
        extracted_brands = set()
        for item in brand_items:
            brand_name = ""
            count = 0
            text_content = item.get_text(separator=' ', strip=True)
            match = re.search(r'^(.*?)\s*\(?(\d+)\)?$', text_content)
            if match:
                brand_name = match.group(1).strip()
                count = int(match.group(2))
                if brand_name.lower() not in ['no', 'yes', ''] and brand_name not in extracted_brands:
                    brands_data.append({'Marka': brand_name,'ÃœrÃ¼n SayÄ±sÄ±': count})
                    extracted_brands.add(brand_name)
            else:
                brand_span = item.find(['label','span'], class_=re.compile(r'label|name', re.IGNORECASE))
                count_span = item.find('span', class_=re.compile(r'count|hitcount', re.IGNORECASE))
                if brand_span and count_span:
                     brand_name = brand_span.get_text(strip=True)
                     count_text = count_span.get_text(strip=True).strip('()[]')
                     if count_text.isdigit():
                         if brand_name.lower() not in ['no', 'yes', ''] and brand_name not in extracted_brands:
                             brands_data.append({'Marka': brand_name, 'ÃœrÃ¼n SayÄ±sÄ±': int(count_text)})
                             extracted_brands.add(brand_name)

        if brands_data:
            st.success(f"{len(brands_data)} marka verisi doÄŸrudan HTML'den baÅŸarÄ±yla Ã§ekildi (Alternatif YÃ¶ntem).")
            return pd.DataFrame(brands_data)
        else:
            st.warning("DoÄŸrudan HTML taramasÄ±nda yapÄ±sal marka verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
            return None
    except Exception as e:
        st.error(f"MarkalarÄ± doÄŸrudan HTML'den Ã§ekerken hata oluÅŸtu (Alternatif YÃ¶ntem): {e}")
        return None


def extract_brands_from_html(html_content):
    """HTML iÃ§eriÄŸinden marka verilerini Ã§Ä±karÄ±r (__NEXT_DATA__ Ã¶ncelikli)."""
    # ... (Bu fonksiyon Ã¶ncekiyle aynÄ±) ...
    if not html_content:
        st.error("HTML iÃ§eriÄŸi alÄ±namadÄ±.")
        return None

    soup = BeautifulSoup(html_content, 'lxml')
    brands_data = []

    script_tag = soup.find('script', id='__NEXT_DATA__')
    if script_tag:
        st.info("__NEXT_DATA__ script'i bulundu, iÅŸleniyor...")
        try:
            next_data = json.loads(script_tag.string)
            brand_filter = find_brand_filter(next_data)

            if brand_filter and 'values' in brand_filter and isinstance(brand_filter['values'], list):
                processed_brands = set()
                for item in brand_filter['values']:
                    if isinstance(item, dict) and 'label' in item and 'hitCount' in item and item['label'] and item['hitCount'] is not None:
                        brand_name = item['label'].strip()
                        if brand_name.lower() != 'no' and brand_name not in processed_brands:
                             brands_data.append({'Marka': brand_name,'ÃœrÃ¼n SayÄ±sÄ±': item['hitCount']})
                             processed_brands.add(brand_name)
                if brands_data:
                     st.success(f"{len(brands_data)} marka verisi __NEXT_DATA__ iÃ§inden baÅŸarÄ±yla Ã§ekildi.")
                     return pd.DataFrame(brands_data)
                else:
                     st.warning("__NEXT_DATA__ iÃ§inde marka verisi bulundu ancak iÅŸlenebilir formatta deÄŸildi veya sadece 'No' etiketleri vardÄ±. Alternatif yÃ¶ntem deneniyor...")
            else:
                st.warning("__NEXT_DATA__ iÃ§inde 'c_brand' attributeId'li veya 'values' listesi iÃ§eren geÃ§erli yapÄ± bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")
        except Exception as e:
            st.error(f"__NEXT_DATA__ iÅŸlenirken hata: {e}. Alternatif yÃ¶ntem deneniyor...")
    else:
        st.warning("__NEXT_DATA__ script'i bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")

    if not brands_data:
        return extract_brands_directly(soup)
    else:
        return pd.DataFrame(brands_data)

# --- Streamlit ArayÃ¼zÃ¼ ---
url = st.text_input("Sephora ÃœrÃ¼n Listeleme SayfasÄ± URL'sini Girin:", placeholder="https://www.sephora.me/ae-en/shop/fragrance/C301")
process_button = st.button("MarkalarÄ± Ã‡ek ve CSV OluÅŸtur")

if process_button and url:
    if not url.startswith(('http://', 'https://')) or "sephora." not in url:
         st.warning("LÃ¼tfen geÃ§erli bir Sephora URL'si girin (http:// veya https:// ile baÅŸlamalÄ±dÄ±r).")
    else:
        with st.spinner("Sayfa indiriliyor (cloudscraper) ve markalar Ã§ekiliyor... LÃ¼tfen bekleyin."):
            # Bu sefer cloudscraper ile dene
            html_content = fetch_html_cloudscraper(url)

            if html_content:
                df_brands = extract_brands_from_html(html_content)

                if df_brands is not None and not df_brands.empty:
                    st.subheader("Ã‡ekilen Marka Verileri")
                    st.dataframe(df_brands, use_container_width=True)

                    try:
                        csv_buffer = StringIO()
                        df_brands.to_csv(csv_buffer, index=False, encoding='utf-8')
                        csv_data = csv_buffer.getvalue()
                        try:
                            path_part = url.split('/')[-1].split('?')[0]
                            safe_part = re.sub(r'[\\/*?:"<>|]', "-", path_part)
                            safe_part = safe_part[:50] if len(safe_part) > 50 else safe_part
                            csv_filename = f"sephora_markalar_{safe_part}.csv" if safe_part else "sephora_markalar.csv"
                        except Exception:
                            csv_filename = "sephora_markalar.csv"

                        st.download_button(
                            label="ğŸ’¾ CSV Olarak Ä°ndir",
                            data=csv_data,
                            file_name=csv_filename,
                            mime='text/csv',
                        )
                    except Exception as e:
                        st.error(f"CSV dosyasÄ± oluÅŸturulurken veya indirme butonu hazÄ±rlanÄ±rken hata: {e}")

                elif df_brands is not None and df_brands.empty:
                     st.warning("Marka verisi bulunamadÄ±. URL'yi, filtrelerin gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ kontrol edin veya sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir.")
            # else: fetch_html_cloudscraper iÃ§inde zaten hata mesajÄ± verildi.

elif process_button and not url:
    st.warning("LÃ¼tfen bir URL girin.")

st.markdown("---")
st.caption("Not: Bu uygulama Sephora web sitesinin yapÄ±sÄ±na baÄŸlÄ±dÄ±r. Site gÃ¼ncellenirse veya gÃ¼venlik Ã¶nlemleri (WAF vb.) kullanÄ±lÄ±rsa veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z olabilir.")
