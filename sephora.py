import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import json
from io import StringIO  # CSV indirme iÃ§in gerekli
# import time # Gerekirse gecikme eklemek iÃ§in

# Sayfa BaÅŸlÄ±ÄŸÄ±
st.set_page_config(page_title="Sephora Marka Filtre Ã‡ekici", layout="wide")
st.title("ğŸ’„ Sephora Marka Filtre Veri Ã‡ekici 1")
st.caption("Sephora Ã¼rÃ¼n listeleme sayfasÄ± linkini girerek marka filtresindeki verileri CSV olarak indirin.")

# --- Fonksiyonlar ---

def fetch_html(url):
    """Verilen URL'den HTML iÃ§eriÄŸini Ã§eker (GeliÅŸmiÅŸ Headers ile)."""
    headers = {
        # GÃ¼ncel ve yaygÄ±n bir User-Agent kullanalÄ±m
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
        # TarayÄ±cÄ±larÄ±n genellikle gÃ¶nderdiÄŸi diÄŸer baÅŸlÄ±klar
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'Accept-Language': 'en-US,en;q=0.9,tr;q=0.8', # tr: TÃ¼rkÃ§e tercihini belirtir
        'Accept-Encoding': 'gzip, deflate, br', # SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ iÃ§eriÄŸi kabul ettiÄŸimizi belirtir
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none', # Ä°lk istekte genellikle 'none' olur
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        # Referer eklemek bazen iÅŸe yarayabilir (isteÄŸe baÄŸlÄ±)
        # 'Referer': 'https://www.google.com/'
    }
    try:
        # time.sleep(1) # Ã‡oklu isteklerde rate limiting'e takÄ±lmamak iÃ§in gerekebilir
        # Timeout sÃ¼resini biraz artÄ±rabiliriz
        response = requests.get(url, headers=headers, timeout=20, allow_redirects=True)
        response.raise_for_status() # HTTP hatalarÄ±nÄ± kontrol et (4xx veya 5xx)

        # Ä°Ã§eriÄŸin boÅŸ olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        if not response.content:
             st.warning(f"URL'den boÅŸ iÃ§erik alÄ±ndÄ±: {url}. Sayfa yapÄ±sÄ± veya yÃ¼kleme sorunu olabilir.")
             return None

        # Ä°Ã§eriÄŸi doÄŸru ÅŸekilde decode etmeye Ã§alÄ±ÅŸalÄ±m
        response.encoding = response.apparent_encoding or 'utf-8'
        st.success(f"URL baÅŸarÄ±yla alÄ±ndÄ±: {url}") # BaÅŸarÄ± mesajÄ± eklendi
        return response.text

    except requests.exceptions.Timeout:
        st.error(f"Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ± (Timeout=20s): {url}")
        return None
    except requests.exceptions.HTTPError as e:
         # Sunucudan gelen hatayÄ± daha detaylÄ± gÃ¶sterelim
         st.error(f"HTTP HatasÄ±: {e.response.status_code} {e.response.reason} - URL: {url}")
         # st.error(f"Sunucu YanÄ±tÄ± (ilk 500 karakter): {e.response.text[:500]}...") # Hassas veri iÃ§erebilir, dikkatli kullanÄ±n
         return None
    except requests.exceptions.ConnectionError as e:
        # Bu, bizim aldÄ±ÄŸÄ±mÄ±z hatayÄ± da kapsar
        st.error(f"BaÄŸlantÄ± HatasÄ±: {e}")
        st.info("Bu hata genellikle sunucunun isteÄŸi engellediÄŸi (WAF/gÃ¼venlik Ã¶nlemi), geÃ§ici aÄŸ sorunlarÄ± olduÄŸu veya URL'nin hatalÄ± olduÄŸu anlamÄ±na gelir. LÃ¼tfen URL'yi tarayÄ±cÄ±da kontrol edin ve bir sÃ¼re sonra tekrar deneyin.")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"Genel URL alÄ±nÄ±rken hata oluÅŸtu: {e}")
        return None

def find_brand_filter(data):
    """Recursive olarak JSON/dictionary iÃ§inde 'attributeId': 'c_brand' arar."""
    if isinstance(data, dict):
        if data.get('attributeId') == 'c_brand' and 'values' in data:
            # DeÄŸerlerin listesi boÅŸ deÄŸilse ve iÃ§inde beklenen yapÄ± varsa dÃ¶ndÃ¼r
            if data['values'] and isinstance(data['values'], list) and all(isinstance(item, dict) for item in data['values']):
                 return data
        for key, value in data.items():
            # 'image' veya benzeri bÃ¼yÃ¼k/gereksiz alanlarÄ± atla (performans iÃ§in)
            if key not in ['image', 'images', 'icon', 'icons', 'banner', 'banners']:
                result = find_brand_filter(value)
                if result:
                    return result
    elif isinstance(data, list):
        for item in data:
            result = find_brand_filter(item)
            if result:
                return result
    return None

def extract_brands_directly(soup):
    """
    Alternatif yÃ¶ntem: DoÄŸrudan HTML elementlerini parse etmeye Ã§alÄ±ÅŸÄ±r.
    Bu yÃ¶ntem sayfa yapÄ±sÄ± deÄŸiÅŸirse kolayca bozulabilir.
    """
    st.info("Alternatif yÃ¶ntem: HTML elementleri taranÄ±yor...")
    brands_data = []
    try:
        # Genellikle filtreler bir tÃ¼r 'facet' veya 'filter' container'Ä± iÃ§inde bulunur.
        # 'Brands' baÅŸlÄ±ÄŸÄ±nÄ± iÃ§eren div'i veya h3'Ã¼ bulup onun parent'Ä±na bakmak iyi bir baÅŸlangÄ±Ã§ olabilir.
        # TarayÄ±cÄ± geliÅŸtirici araÃ§larÄ±yla spesifik class'larÄ± veya ID'leri belirlemek en doÄŸrusu.
        brand_header = soup.find(['h3', 'h2', 'div'], string=re.compile(r'Brands', re.IGNORECASE))

        if not brand_header:
            st.warning("DoÄŸrudan HTML'de 'Brands' baÅŸlÄ±ÄŸÄ± bulunamadÄ±. Daha genel filtre container'larÄ± aranÄ±yor...")
            # Genel filtre container'larÄ±nÄ± ara
            filter_containers = soup.find_all('div', class_=re.compile(r'filter-section|facet-container|refinement-options', re.IGNORECASE))
            if not filter_containers:
                 st.error("Marka filtresi bÃ¶lÃ¼mÃ¼ HTML iÃ§inde bulunamadÄ± (Alternatif YÃ¶ntem).")
                 return None
            # Ä°Ã§inde checkbox input olan ilk container'Ä± seÃ§meye Ã§alÄ±ÅŸalÄ±m
            brand_section = None
            for container in filter_containers:
                if container.find('input', type='checkbox', attrs={'name': re.compile(r'brand|Brand', re.IGNORECASE)}):
                    brand_section = container
                    st.info(f"OlasÄ± marka filtre container'Ä± bulundu: {container.get('class', 'N/A')}")
                    break
            if not brand_section:
                st.error("Checkbox iÃ§eren marka filtresi container'Ä± bulunamadÄ±.")
                return None
        else:
             # BaÅŸlÄ±ktan yola Ã§Ä±karak parent elementi bul
             # Bu yapÄ± siteye gÃ¶re Ã§ok deÄŸiÅŸir (Ã¶rnek: baÅŸlÄ±ÄŸÄ±n 2 Ã¼st parent'Ä± olabilir)
             brand_section = brand_header.find_parent('div', class_=re.compile(r'filter-section|facet-container|refinement-options', re.IGNORECASE))
             if not brand_section:
                 brand_section = brand_header.find_parent('div').find_parent('div') # Ã–rnek deneme
             if not brand_section:
                 st.error("Marka baÅŸlÄ±ÄŸÄ±ndan yola Ã§Ä±karak filtre bÃ¶lÃ¼mÃ¼ bulunamadÄ±.")
                 return None
             st.info(f"Marka baÅŸlÄ±ÄŸÄ±ndan yola Ã§Ä±karak filtre container'Ä± bulundu: {brand_section.get('class', 'N/A')}")


        # Marka checkbox'larÄ±nÄ± veya bunlarÄ± iÃ§eren div'leri bul
        # Class isimleri Ã§ok deÄŸiÅŸkendir, regex ile daha esnek arama yapÄ±yoruz
        brand_items = brand_section.find_all('div', class_=re.compile(r'checkbox-item|facet-value|filter-value', re.IGNORECASE))

        if not brand_items:
             # Belki de doÄŸrudan label iÃ§indedirler
             brand_items = brand_section.find_all('label', class_=re.compile(r'checkbox-label|facet-label', re.IGNORECASE))

        if not brand_items:
            st.error("Marka listesi elementleri (item/label) bulunamadÄ±.")
            return None

        st.info(f"Bulunan olasÄ± marka elementi sayÄ±sÄ±: {len(brand_items)}")

        for item in brand_items:
            brand_name = ""
            count = 0

            # Label iÃ§indeki metni dene
            text_content = item.get_text(separator=' ', strip=True)

            # Regex ile "Marka AdÄ± (SayÄ±)" formatÄ±nÄ± ara
            match = re.search(r'^(.*?)\s*\((\d+)\)$', text_content)
            if match:
                brand_name = match.group(1).strip()
                count = int(match.group(2))
                if brand_name == "No": # "No (2048)" gibi durumlarÄ± elemek iÃ§in
                    continue
                brands_data.append({
                    'Marka': brand_name,
                    'ÃœrÃ¼n SayÄ±sÄ±': count
                })
            else:
                # EÅŸleÅŸme olmazsa, belki sayÄ± ayrÄ± bir span iÃ§indedir (daha az olasÄ±)
                brand_span = item.find('span', class_=re.compile(r'label|name', re.IGNORECASE))
                count_span = item.find('span', class_=re.compile(r'count|hitcount', re.IGNORECASE))
                if brand_span and count_span:
                     brand_name = brand_span.get_text(strip=True)
                     count_text = count_span.get_text(strip=True).strip('()')
                     if count_text.isdigit():
                         if brand_name == "No": continue
                         brands_data.append({
                            'Marka': brand_name,
                            'ÃœrÃ¼n SayÄ±sÄ±': int(count_text)
                         })

        if brands_data:
            st.success(f"{len(brands_data)} marka verisi doÄŸrudan HTML'den baÅŸarÄ±yla Ã§ekildi (Alternatif YÃ¶ntem).")
            return pd.DataFrame(brands_data)
        else:
            st.warning("DoÄŸrudan HTML taramasÄ±nda yapÄ±sal marka verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
            return None

    except Exception as e:
        st.error(f"MarkalarÄ± doÄŸrudan HTML'den Ã§ekerken hata oluÅŸtu (Alternatif YÃ¶ntem): {e}")
        return None


def extract_brands_from_html(html_content):
    """
    HTML iÃ§eriÄŸinden __NEXT_DATA__ script'ini bularak marka verilerini Ã§Ä±karÄ±r.
    Sephora'nÄ±n Next.js kullandÄ±ÄŸÄ± ve veriyi script tag'ine gÃ¶mdÃ¼ÄŸÃ¼ varsayÄ±lÄ±r.
    """
    if not html_content:
        return None

    soup = BeautifulSoup(html_content, 'lxml') # lxml parser kullanÄ±yoruz
    brands_data = []

    # Next.js'in veri gÃ¶mdÃ¼ÄŸÃ¼ script tag'ini bul
    script_tag = soup.find('script', id='__NEXT_DATA__')

    if not script_tag:
        st.warning("Sayfa yapÄ±sÄ± beklenenden farklÄ±. __NEXT_DATA__ script'i bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")
        return extract_brands_directly(soup) # Alternatif yÃ¶nteme geÃ§

    try:
        # Script iÃ§eriÄŸini JSON olarak parse et
        next_data = json.loads(script_tag.string)

        # Ã‡ok katmanlÄ± JSON iÃ§inde 'attributeId': 'c_brand' iÃ§eren objeyi bul
        # 'props.pageProps.initialState...' gibi yollar yerine genel arama yapÄ±yoruz
        brand_filter = find_brand_filter(next_data)

        if brand_filter and 'values' in brand_filter and isinstance(brand_filter['values'], list):
            for item in brand_filter['values']:
                # Ã–ÄŸenin dictionary olduÄŸunu ve gerekli anahtarlarÄ± iÃ§erdiÄŸini kontrol et
                if isinstance(item, dict) and 'label' in item and 'hitCount' in item and item['label'] and item['hitCount'] is not None:
                    # "No" gibi anlamsÄ±z etiketleri filtrele (hitCount'a gÃ¶re deÄŸil, isme gÃ¶re)
                    if item['label'].strip().lower() != 'no':
                         brands_data.append({
                            'Marka': item['label'],
                            'ÃœrÃ¼n SayÄ±sÄ±': item['hitCount']
                         })
            if brands_data:
                 st.success(f"{len(brands_data)} marka verisi __NEXT_DATA__ iÃ§inden baÅŸarÄ±yla Ã§ekildi.")
                 return pd.DataFrame(brands_data)
            else:
                 st.warning("__NEXT_DATA__ iÃ§inde marka verisi bulundu ancak iÅŸlenebilir formatta deÄŸildi veya 'No' etiketleri vardÄ±. Alternatif yÃ¶ntem deneniyor...")
                 return extract_brands_directly(soup) # Alternatif yÃ¶nteme geÃ§
        else:
            st.warning("__NEXT_DATA__ iÃ§inde 'c_brand' attributeId'li veya 'values' listesi iÃ§eren geÃ§erli yapÄ± bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")
            return extract_brands_directly(soup) # Alternatif yÃ¶nteme geÃ§

    except (json.JSONDecodeError, KeyError, TypeError) as e:
        st.error(f"__NEXT_DATA__ parse edilirken veya iÅŸlenirken hata: {e}. Alternatif yÃ¶ntem deneniyor...")
        return extract_brands_directly(soup) # Alternatif yÃ¶nteme geÃ§
    except Exception as e:
         st.error(f"MarkalarÄ± Ã§ekerken beklenmedik bir hata oluÅŸtu: {e}. Alternatif yÃ¶ntem deneniyor...")
         return extract_brands_directly(soup) # Alternatif yÃ¶nteme geÃ§


# --- Streamlit ArayÃ¼zÃ¼ ---
url = st.text_input("Sephora ÃœrÃ¼n Listeleme SayfasÄ± URL'sini Girin:", placeholder="https://www.sephora.ae/en/shop/makeup/C302")
process_button = st.button("MarkalarÄ± Ã‡ek ve CSV OluÅŸtur")

if process_button and url:
    # Basit URL doÄŸrulamasÄ±
    if not url.startswith(('http://', 'https://')) or "sephora." not in url:
         st.warning("LÃ¼tfen geÃ§erli bir Sephora URL'si girin (http:// veya https:// ile baÅŸlamalÄ±dÄ±r).")
    else:
        with st.spinner("Sayfa indiriliyor ve markalar Ã§ekiliyor... LÃ¼tfen bekleyin."):
            html_content = fetch_html(url)

            if html_content:
                df_brands = extract_brands_from_html(html_content)

                if df_brands is not None and not df_brands.empty:
                    st.subheader("Ã‡ekilen Marka Verileri")
                    st.dataframe(df_brands, use_container_width=True)

                    # CSV Ä°ndirme Butonu
                    try:
                        csv_buffer = StringIO()
                        # UTF-8 BOM (Byte Order Mark) ekleyerek Excel uyumluluÄŸunu artÄ±rabiliriz
                        # df_brands.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
                        df_brands.to_csv(csv_buffer, index=False, encoding='utf-8')
                        csv_data = csv_buffer.getvalue()

                        # URL'den dosya adÄ± oluÅŸturma (daha saÄŸlam)
                        try:
                            # URL'nin son kÄ±smÄ±nÄ± al (sorgu parametrelerini kaldÄ±r)
                            path_part = url.split('/')[-1].split('?')[0]
                            # GeÃ§ersiz karakterleri kaldÄ±r veya deÄŸiÅŸtir
                            safe_part = re.sub(r'[\\/*?:"<>|]', "-", path_part)
                            # Ã‡ok uzunsa kÄ±salt
                            safe_part = safe_part[:50] if len(safe_part) > 50 else safe_part
                            csv_filename = f"sephora_markalar_{safe_part}.csv" if safe_part else "sephora_markalar.csv"
                        except Exception:
                            csv_filename = "sephora_markalar.csv"


                        st.download_button(
                            label="ğŸ’¾ CSV Olarak Ä°ndir",
                            data=csv_data,
                            file_name=csv_filename,
                            mime='text/csv',
                        )
                    except Exception as e:
                        st.error(f"CSV dosyasÄ± oluÅŸturulurken veya indirme butonu hazÄ±rlanÄ±rken hata: {e}")

                elif df_brands is not None and df_brands.empty:
                     st.warning("Marka verisi bulunamadÄ±. URL'yi, filtrelerin gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ kontrol edin veya sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir.")
                # else: extract_brands_from_html iÃ§inde zaten hata mesajÄ± verildi.
            # else: fetch_html iÃ§inde zaten hata mesajÄ± verildi.

elif process_button and not url:
    st.warning("LÃ¼tfen bir URL girin.")

st.markdown("---")
st.caption("Not: Bu uygulama Sephora web sitesinin yapÄ±sÄ±na baÄŸlÄ±dÄ±r. Site gÃ¼ncellenirse veya gÃ¼venlik Ã¶nlemleri artÄ±rÄ±lÄ±rsa Ã§alÄ±ÅŸmayabilir.")
