import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import re
import json
from io import StringIO
import os

# Sayfa BaÅŸlÄ±ÄŸÄ±
st.set_page_config(page_title="Sephora Marka Filtre Ã‡ekici", layout="wide")
st.title("ğŸ’„ Sephora Marka Filtre Veri Ã‡ekici (HTML YÃ¼kleme)")
st.caption("KaydettiÄŸiniz Sephora Ã¼rÃ¼n listeleme sayfasÄ± HTML dosyasÄ±nÄ± yÃ¼kleyerek marka filtresindeki verileri CSV olarak indirin.")

# --- KullanÄ±cÄ± TalimatlarÄ± ---
st.info("""
**NasÄ±l KullanÄ±lÄ±r:**
1.  Marka filtrelerini Ã§ekmek istediÄŸiniz Sephora Ã¼rÃ¼n listeleme sayfasÄ±nÄ± (Ã¶rn: Makyaj, ParfÃ¼m kategorisi) **web tarayÄ±cÄ±nÄ±zda** aÃ§Ä±n.
2.  SayfanÄ±n **tamamen** yÃ¼klendiÄŸinden emin olun (sol taraftaki **"Refine"** veya benzeri bÃ¶lÃ¼mdeki **"Brands"** filtresinin ve markalarÄ±n gÃ¶rÃ¼nÃ¼r olduÄŸundan emin olun).
3.  TarayÄ±cÄ±da sayfaya saÄŸ tÄ±klayÄ±n ve **"FarklÄ± Kaydet" (Save Page As...)** seÃ§eneÄŸini seÃ§in.
4.  KayÄ±t tÃ¼rÃ¼ olarak **"Web SayfasÄ±, Sadece HTML" (Webpage, HTML Only)** veya **"Web SayfasÄ±, TamamÄ±" (Webpage, Complete)** seÃ§eneklerinden birini seÃ§in. **"Sadece HTML" genellikle daha iyidir.** Dosya uzantÄ±sÄ± `.html` veya `.htm` olmalÄ±dÄ±r.
5.  KaydettiÄŸiniz bu `.html` dosyasÄ±nÄ± aÅŸaÄŸÄ±daki "GÃ¶zat" dÃ¼ÄŸmesini kullanarak yÃ¼kleyin.
""")

# --- Fonksiyonlar ---

def find_nested_data(data, target_key, target_value):
    """
    Ä°Ã§ iÃ§e geÃ§miÅŸ dictionary ve listelerde belirli bir anahtar-deÄŸer Ã§iftini
    iÃ§eren dictionary'yi arar.
    """
    if isinstance(data, dict):
        if data.get(target_key) == target_value:
            return data
        for key, value in data.items():
            # Ã‡ok bÃ¼yÃ¼k veya alakasÄ±z dallarÄ± budama (opsiyonel, dikkatli kullanÄ±lmalÄ±)
            # if key in ['big_list_key', 'unrelated_data']: continue
            found = find_nested_data(value, target_key, target_value)
            if found:
                return found
    elif isinstance(data, list):
        for item in data:
            found = find_nested_data(item, target_key, target_value)
            if found:
                return found
    return None

def extract_brands_from_next_data(script_content):
    """__NEXT_DATA__ script iÃ§eriÄŸinden marka verilerini Ã§Ä±karÄ±r."""
    brands_data = []
    processed_brands = set()
    try:
        next_data = json.loads(script_content)
        st.success("__NEXT_DATA__ JSON olarak baÅŸarÄ±yla parse edildi.")

        # 'attributeId': 'c_brand' iÃ§eren dictionary'yi bulmaya Ã§alÄ±ÅŸ
        # Daha derinlerde olabilir, genel arama yapalÄ±m
        brand_filter_dict = find_nested_data(next_data, 'attributeId', 'c_brand')

        if brand_filter_dict:
            st.success("'c_brand' attributeId iÃ§eren yapÄ± bulundu.")
            if 'values' in brand_filter_dict and isinstance(brand_filter_dict['values'], list):
                st.info(f"Marka 'values' listesinde {len(brand_filter_dict['values'])} Ã¶ÄŸe bulundu.")
                for item in brand_filter_dict['values']:
                    if isinstance(item, dict) and 'label' in item and 'hitCount' in item:
                        brand_name = item['label'].strip()
                        hit_count = item['hitCount']
                        # GeÃ§erli veri kontrolÃ¼
                        if brand_name and isinstance(hit_count, int) and brand_name.lower() != 'no' and brand_name not in processed_brands:
                            brands_data.append({'Marka': brand_name, 'ÃœrÃ¼n SayÄ±sÄ±': hit_count})
                            processed_brands.add(brand_name)
                if brands_data:
                    st.info(f"{len(brands_data)} geÃ§erli marka/sayÄ± Ã§ifti ayÄ±klandÄ±.")
                    return pd.DataFrame(brands_data)
                else:
                    st.warning("Marka filtresi ('c_brand') bulundu ancak iÃ§inde geÃ§erli 'label' ve 'hitCount' iÃ§eren Ã¶ÄŸe bulunamadÄ±.")
                    return pd.DataFrame() # BoÅŸ DataFrame
            else:
                st.warning("'c_brand' yapÄ±sÄ± bulundu ancak geÃ§erli bir 'values' listesi iÃ§ermiyor.")
                # st.json(brand_filter_dict) # Bulunan yapÄ±yÄ± gÃ¶ster (debug)
                return None
        else:
            st.warning("__NEXT_DATA__ iÃ§inde 'c_brand' attributeId'li filtre yapÄ±sÄ± bulunamadÄ±.")
            # Anahtar yapÄ±larÄ± gÃ¶rmek iÃ§in verinin bir kÄ±smÄ±nÄ± yazdÄ±rabiliriz (debug)
            # if 'props' in next_data and 'pageProps' in next_data['props']:
            #     st.json(list(next_data['props']['pageProps'].keys()))
            return None

    except json.JSONDecodeError as e:
        st.error(f"__NEXT_DATA__ iÃ§eriÄŸi JSON olarak ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {e}")
        st.text("HTML dosyasÄ±nÄ±n 'Sadece HTML' olarak kaydedildiÄŸinden emin olun.")
        return None
    except Exception as e:
        st.error(f"__NEXT_DATA__ iÅŸlenirken beklenmedik bir hata oluÅŸtu: {e}")
        return None


def extract_brands_directly(soup):
    """Alternatif yÃ¶ntem: DoÄŸrudan HTML elementlerini parse etmeye Ã§alÄ±ÅŸÄ±r (Daha Basit)."""
    st.info("Alternatif yÃ¶ntem (HTML elementleri) deneniyor...")
    brands_data = []
    processed_brands = set()

    # Filtre bÃ¶lÃ¼mÃ¼nÃ¼ bulmak iÃ§in daha genel bir yaklaÅŸÄ±m deneyelim
    # Ä°Ã§inde 'Refine', 'Filter', 'Brands' gibi kelimeler geÃ§en ve liste iÃ§eren yapÄ±lar ara
    possible_sections = soup.find_all(['div', 'aside', 'section'], class_=re.compile(r'filter|facet|refine', re.IGNORECASE))
    if not possible_sections:
        # ID'leri deneyelim
         possible_sections = soup.find_all(id=re.compile(r'filter|facet', re.IGNORECASE))

    brand_section = None
    for section in possible_sections:
         # Ä°Ã§inde 'brand' kelimesi geÃ§en bir input veya Ã§ok sayÄ±da potansiyel filtre deÄŸeri var mÄ±?
         # Veya doÄŸrudan 'Brands' baÅŸlÄ±ÄŸÄ±nÄ± iÃ§eriyor mu?
         if section.find(['h3','h2','button'], string=re.compile(r'Brands?', re.IGNORECASE)) or \
            section.find('input', attrs={'name': re.compile(r'brand', re.IGNORECASE)}) or \
            len(section.find_all(['label', 'li', 'div'], class_=re.compile(r'checkbox|facet-value|option', re.IGNORECASE))) > 5:
                brand_section = section
                st.info(f"Potansiyel filtre bÃ¶lÃ¼mÃ¼ bulundu: <{brand_section.name} class='{brand_section.get('class', 'N/A')}' id='{brand_section.get('id', 'N/A')}'>")
                break # Ä°lk uygun olanÄ± al

    if not brand_section:
        st.error("Marka filtresi bÃ¶lÃ¼mÃ¼ HTML iÃ§inde otomatik olarak tespit edilemedi.")
        return None

    # Marka listesi Ã¶ÄŸelerini (genellikle label veya onu iÃ§eren div/li) bul
    # Class isimleri Ã§ok deÄŸiÅŸken olabilir, yapÄ±sal olarak arayalÄ±m
    # Ä°Ã§inde checkbox olan label'larÄ± veya checkbox'Ä±n parent'larÄ±nÄ± hedefleyebiliriz
    items = brand_section.find_all('label', class_=re.compile(r'checkbox|facet', re.IGNORECASE))
    if not items:
        inputs = brand_section.find_all('input', type='checkbox', attrs={'name': re.compile(r'brand', re.IGNORECASE)})
        items = [inp.find_parent(['label','div','li']) for inp in inputs if inp.find_parent(['label','div','li'])]

    if not items:
        # Daha genel bir arama: iÃ§inde sayÄ± olan parantez iÃ§eren herhangi bir liste Ã¶ÄŸesi
        items = brand_section.find_all(['li','div'], text=re.compile(r'\(\d+\)\s*$'))


    if not items:
        st.error("Marka listesi elementleri bulunamadÄ±.")
        return None

    st.info(f"Bulunan olasÄ± marka elementi sayÄ±sÄ±: {len(items)}")

    for item in items:
        text_content = item.get_text(separator=' ', strip=True)
        # Regex: Marka AdÄ± (SayÄ±) formatÄ±nÄ± ara (daha toleranslÄ±)
        match = re.search(r'^(.*?)\s*\((\d+)\)$', text_content)
        if match:
            brand_name = match.group(1).strip()
            count = int(match.group(2))
            # Temel filtreleme
            if brand_name and brand_name.lower() not in ['no', 'yes'] and brand_name not in processed_brands:
                brands_data.append({'Marka': brand_name, 'ÃœrÃ¼n SayÄ±sÄ±': count})
                processed_brands.add(brand_name)

    if brands_data:
        st.success(f"{len(brands_data)} marka verisi doÄŸrudan HTML'den baÅŸarÄ±yla Ã§ekildi.")
        return pd.DataFrame(brands_data)
    else:
        st.warning("DoÄŸrudan HTML taramasÄ±nda yapÄ±sal marka verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
        return None


# --- Streamlit ArayÃ¼zÃ¼ ---
uploaded_file = st.file_uploader(
    "KaydedilmiÅŸ Sephora HTML DosyasÄ±nÄ± YÃ¼kleyin (.html/.htm)",
    type=["html", "htm"],
    accept_multiple_files=False
)

if uploaded_file is not None:
    st.success(f"'{uploaded_file.name}' baÅŸarÄ±yla yÃ¼klendi.")
    with st.spinner("HTML dosyasÄ± okunuyor ve markalar ayrÄ±ÅŸtÄ±rÄ±lÄ±yor..."):
        df_brands = None # BaÅŸlangÄ±Ã§ta DataFrame'i None yapalÄ±m
        html_content = None
        try:
            # Dosya iÃ§eriÄŸini oku ve decode et
            html_content = uploaded_file.getvalue().decode("utf-8")
            st.info("HTML iÃ§eriÄŸi okundu.")
        except UnicodeDecodeError:
            st.error("Dosya UTF-8 formatÄ±nda okunamadÄ±. LÃ¼tfen dosyayÄ± tarayÄ±cÄ±dan 'FarklÄ± Kaydet -> Web SayfasÄ±, Sadece HTML' seÃ§eneÄŸi ile tekrar kaydedip deneyin.")
        except Exception as e:
             st.error(f"Dosya okunurken hata oluÅŸtu: {e}")

        if html_content:
            # Ã–nce __NEXT_DATA__ dene
            df_brands = extract_brands_from_next_data(html_content)

            # __NEXT_DATA__ baÅŸarÄ±sÄ±z olursa veya boÅŸ dÃ¶nerse alternatif yÃ¶ntemi dene
            if df_brands is None or df_brands.empty:
                 st.info("__NEXT_DATA__ baÅŸarÄ±sÄ±z oldu veya veri bulunamadÄ±, doÄŸrudan HTML deneniyor.")
                 try:
                     soup_alt = BeautifulSoup(html_content, 'lxml')
                     df_brands = extract_brands_directly(soup_alt)
                 except Exception as e:
                     st.error(f"Alternatif HTML ayrÄ±ÅŸtÄ±rma yÃ¶nteminde hata oluÅŸtu: {e}")
                     df_brands = None # Hata durumunda None yap

            # Sonucu gÃ¶ster ve CSV indir
            if df_brands is not None and not df_brands.empty:
                st.subheader("Ã‡ekilen Marka Verileri")
                st.dataframe(df_brands.set_index('Marka'), use_container_width=True)
                # --- CSV Ä°ndirme ---
                try:
                    csv_buffer = StringIO()
                    df_brands.to_csv(csv_buffer, index=False, encoding='utf-8')
                    csv_data = csv_buffer.getvalue()
                    base_filename = os.path.splitext(uploaded_file.name)[0]
                    csv_filename = f"sephora_markalar_{base_filename}.csv"
                    st.download_button(
                        label="ğŸ’¾ CSV Olarak Ä°ndir",
                        data=csv_data,
                        file_name=csv_filename,
                        mime='text/csv',
                    )
                except Exception as e:
                    st.error(f"CSV oluÅŸturulurken/indirilirken hata: {e}")
            elif df_brands is not None: # BoÅŸ DataFrame geldiyse
                 st.warning("YÃ¼klenen HTML dosyasÄ±nda, her iki yÃ¶ntemle de (__NEXT_DATA__ veya doÄŸrudan HTML tarama) marka filtresi verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
            # else: Hata zaten yukarÄ±da gÃ¶sterildi.


st.markdown("---")
st.caption("Not: Bu uygulama, yÃ¼klediÄŸiniz HTML dosyasÄ±nÄ±n iÃ§indeki verilere dayanÄ±r. En iyi sonuÃ§ iÃ§in sayfayÄ± tarayÄ±cÄ±da **tamamen yÃ¼klendikten sonra** 'FarklÄ± Kaydet -> Web SayfasÄ±, Sadece HTML' olarak kaydedin.")
