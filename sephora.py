import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import json
from io import StringIO
import time # KÃ¼Ã§Ã¼k bir gecikme ekleyebiliriz

# Sayfa BaÅŸlÄ±ÄŸÄ±
st.set_page_config(page_title="Sephora Marka Filtre Ã‡ekici", layout="wide")
st.title("ğŸ’„ Sephora Marka Filtre Veri Ã‡ekici")
st.caption("Sephora Ã¼rÃ¼n listeleme sayfasÄ± linkini girerek marka filtresindeki verileri CSV olarak indirin.")

# --- Fonksiyonlar ---

@st.cache_resource # Session objesini cache'le
def get_session():
    """Tek bir requests.Session nesnesi oluÅŸturur."""
    session = requests.Session()
    # Session iÃ§in varsayÄ±lan header'larÄ± ayarla
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', # Daha gÃ¼ncel bir UA
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,tr-TR;q=0.8,tr;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Sec-Ch-Ua': '"Not.A/Brand";v="8", "Chromium";v="114", "Google Chrome";v="114"', # Client Hints
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"',
        'Cache-Control': 'max-age=0',
    }
    session.headers.update(headers)
    return session

def fetch_html_requests(url):
    """Verilen URL'den HTML iÃ§eriÄŸini requests.Session ile Ã§eker."""
    session = get_session()
    try:
        # time.sleep(0.5) # Ã‡ok nazik bir gecikme
        response = session.get(url, timeout=20, allow_redirects=True) # Session objesi ile get isteÄŸi
        response.raise_for_status()

        if not response.content:
             st.warning(f"URL'den boÅŸ iÃ§erik alÄ±ndÄ±: {url}. Sayfa yapÄ±sÄ± veya engelleme sorunu olabilir.")
             return None

        # Ä°Ã§eriÄŸi doÄŸru ÅŸekilde decode et
        response.encoding = response.apparent_encoding or 'utf-8'
        st.success(f"URL baÅŸarÄ±yla alÄ±ndÄ± (requests): {url}")
        return response.text

    except requests.exceptions.Timeout:
        st.error(f"Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ± (Timeout=20s): {url}")
        return None
    except requests.exceptions.HTTPError as e:
         st.error(f"HTTP HatasÄ±: {e.response.status_code} {e.response.reason} - URL: {url}")
         # st.error(f"Sunucu YanÄ±tÄ± (ilk 500 karakter): {e.response.text[:500]}...")
         if e.response.status_code == 403:
              st.warning("403 Forbidden hatasÄ± alÄ±ndÄ±. Bu genellikle bot korumasÄ± anlamÄ±na gelir.")
         elif e.response.status_code >= 500:
              st.warning("Sunucu hatasÄ± alÄ±ndÄ±. Site geÃ§ici olarak eriÅŸilemez olabilir.")
         return None
    except requests.exceptions.ConnectionError as e:
        st.error(f"BaÄŸlantÄ± HatasÄ±: {e}")
        st.info("Bu hata genellikle sunucunun isteÄŸi engellediÄŸi, geÃ§ici aÄŸ sorunlarÄ± olduÄŸu veya URL'nin hatalÄ± olduÄŸu anlamÄ±na gelir. LÃ¼tfen URL'yi tarayÄ±cÄ±da kontrol edin.")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"Genel URL alÄ±nÄ±rken hata oluÅŸtu: {e}")
        return None

# --- Marka Ã‡Ä±karma FonksiyonlarÄ± (Ã–nceki kodla aynÄ±) ---

def find_brand_filter(data):
    """Recursive olarak JSON/dictionary iÃ§inde 'attributeId': 'c_brand' arar."""
    if isinstance(data, dict):
        if data.get('attributeId') == 'c_brand' and 'values' in data:
            if data['values'] and isinstance(data['values'], list) and all(isinstance(item, dict) for item in data['values']):
                 return data
        for key, value in data.items():
            if key not in ['image', 'images', 'icon', 'icons', 'banner', 'banners']:
                result = find_brand_filter(value)
                if result:
                    return result
    elif isinstance(data, list):
        for item in data:
            result = find_brand_filter(item)
            if result:
                return result
    return None

def extract_brands_directly(soup):
    """Alternatif yÃ¶ntem: DoÄŸrudan HTML elementlerini parse etmeye Ã§alÄ±ÅŸÄ±r."""
    st.info("Alternatif yÃ¶ntem: HTML elementleri taranÄ±yor...")
    brands_data = []
    try:
        brand_header = soup.find(['h3', 'h2', 'div'], string=re.compile(r'Brands', re.IGNORECASE))
        if not brand_header:
            st.warning("DoÄŸrudan HTML'de 'Brands' baÅŸlÄ±ÄŸÄ± bulunamadÄ±. Daha genel filtre container'larÄ± aranÄ±yor...")
            filter_containers = soup.find_all('div', class_=re.compile(r'filter-section|facet-container|refinement-options', re.IGNORECASE))
            if not filter_containers:
                 st.error("Marka filtresi bÃ¶lÃ¼mÃ¼ HTML iÃ§inde bulunamadÄ± (Alternatif YÃ¶ntem).")
                 return None
            brand_section = None
            for container in filter_containers:
                # Checkbox input'u iÃ§eren bir container ara
                if container.find('input', type='checkbox', attrs={'name': re.compile(r'brand|Brand', re.IGNORECASE)}):
                    brand_section = container
                    st.info(f"OlasÄ± marka filtre container'Ä± bulundu: {container.get('class', 'N/A')}")
                    break
                # Veya doÄŸrudan marka listesi gibi gÃ¶rÃ¼nen bir yapÄ± ara (Ã¶rneÄŸin, Ã§ok sayÄ±da label iÃ§eren)
                elif len(container.find_all('label', class_=re.compile(r'checkbox-label|facet-label', re.IGNORECASE))) > 5: # EÅŸik deÄŸeri ayarlanabilir
                    brand_section = container
                    st.info(f"Label iÃ§eren olasÄ± marka filtre container'Ä± bulundu: {container.get('class', 'N/A')}")
                    break
            if not brand_section:
                 st.error("Checkbox/Label iÃ§eren marka filtresi container'Ä± bulunamadÄ±.")
                 return None
        else:
             brand_section = brand_header.find_parent('div', class_=re.compile(r'filter-section|facet-container|refinement-options', re.IGNORECASE))
             if not brand_section: brand_section = brand_header.find_parent('div', recursive=False) # DoÄŸrudan parent
             if not brand_section: brand_section = brand_header.find_parent('div').find_parent('div') # Ä°ki Ã¼st parent
             if not brand_section:
                 st.error("Marka baÅŸlÄ±ÄŸÄ±ndan yola Ã§Ä±karak filtre bÃ¶lÃ¼mÃ¼ bulunamadÄ±.")
                 return None
             st.info(f"Marka baÅŸlÄ±ÄŸÄ±ndan yola Ã§Ä±karak filtre container'Ä± bulundu: {brand_section.get('class', 'N/A')}")

        # Marka item'larÄ±nÄ±/label'larÄ±nÄ± bul
        brand_items = brand_section.find_all(['div', 'li', 'label'], class_=re.compile(r'checkbox-item|facet-value|filter-value|checkbox-label|facet-label', re.IGNORECASE))

        if not brand_items:
            st.error("Marka listesi elementleri (item/label) bulunamadÄ±.")
            return None

        st.info(f"Bulunan olasÄ± marka elementi sayÄ±sÄ±: {len(brand_items)}")

        for item in brand_items:
            brand_name = ""
            count = 0
            text_content = item.get_text(separator=' ', strip=True)
            match = re.search(r'^(.*?)\s*\((\d+)\)$', text_content) # Marka (SayÄ±) formatÄ±
            if match:
                brand_name = match.group(1).strip()
                count = int(match.group(2))
                if brand_name.lower() == 'no' or not brand_name : continue
                brands_data.append({'Marka': brand_name,'ÃœrÃ¼n SayÄ±sÄ±': count})
            else: # Belki sayÄ± ayrÄ± bir span'de
                 brand_span = item.find(['label','span'], class_=re.compile(r'label|name', re.IGNORECASE))
                 count_span = item.find('span', class_=re.compile(r'count|hitcount', re.IGNORECASE))
                 if brand_span and count_span:
                     brand_name = brand_span.get_text(strip=True)
                     count_text = count_span.get_text(strip=True).strip('()[]') # Parantezleri temizle
                     if count_text.isdigit():
                         if brand_name.lower() == 'no' or not brand_name: continue
                         brands_data.append({'Marka': brand_name, 'ÃœrÃ¼n SayÄ±sÄ±': int(count_text)})

        if brands_data:
            st.success(f"{len(brands_data)} marka verisi doÄŸrudan HTML'den baÅŸarÄ±yla Ã§ekildi (Alternatif YÃ¶ntem).")
            df = pd.DataFrame(brands_data)
            df_unique = df.drop_duplicates(subset=['Marka'])
            st.info(f"Yinelenenler kaldÄ±rÄ±ldÄ±ktan sonra {len(df_unique)} marka kaldÄ±.")
            return df_unique
        else:
            st.warning("DoÄŸrudan HTML taramasÄ±nda yapÄ±sal marka verisi bulunamadÄ± veya ayÄ±klanamadÄ±.")
            return None

    except Exception as e:
        st.error(f"MarkalarÄ± doÄŸrudan HTML'den Ã§ekerken hata oluÅŸtu (Alternatif YÃ¶ntem): {e}")
        return None

def extract_brands_from_html(html_content):
    """HTML iÃ§eriÄŸinden marka verilerini Ã§Ä±karÄ±r (__NEXT_DATA__ Ã¶ncelikli)."""
    if not html_content:
        st.error("HTML iÃ§eriÄŸi alÄ±namadÄ±.")
        return None

    soup = BeautifulSoup(html_content, 'lxml')
    brands_data = []

    script_tag = soup.find('script', id='__NEXT_DATA__')
    if script_tag:
        st.info("__NEXT_DATA__ script'i bulundu, iÅŸleniyor...")
        try:
            next_data = json.loads(script_tag.string)
            brand_filter = find_brand_filter(next_data)

            if brand_filter and 'values' in brand_filter and isinstance(brand_filter['values'], list):
                for item in brand_filter['values']:
                    if isinstance(item, dict) and 'label' in item and 'hitCount' in item and item['label'] and item['hitCount'] is not None:
                        if item['label'].strip().lower() != 'no':
                             brands_data.append({'Marka': item['label'],'ÃœrÃ¼n SayÄ±sÄ±': item['hitCount']})
                if brands_data:
                     st.success(f"{len(brands_data)} marka verisi __NEXT_DATA__ iÃ§inden baÅŸarÄ±yla Ã§ekildi.")
                     return pd.DataFrame(brands_data)
                else:
                     st.warning("__NEXT_DATA__ iÃ§inde marka verisi bulundu ancak iÅŸlenebilir formatta deÄŸildi veya sadece 'No' etiketleri vardÄ±. Alternatif yÃ¶ntem deneniyor...")
            else:
                st.warning("__NEXT_DATA__ iÃ§inde 'c_brand' attributeId'li veya 'values' listesi iÃ§eren geÃ§erli yapÄ± bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")
        except Exception as e:
            st.error(f"__NEXT_DATA__ iÅŸlenirken hata: {e}. Alternatif yÃ¶ntem deneniyor...")
    else:
        st.warning("__NEXT_DATA__ script'i bulunamadÄ±. Alternatif yÃ¶ntem deneniyor...")

    # __NEXT_DATA__'dan veri alÄ±namazsa veya boÅŸsa, doÄŸrudan HTML'den Ã§ekmeyi dene
    if not brands_data:
        return extract_brands_directly(soup)
    else: # __NEXT_DATA__ bulundu ama boÅŸtu
        return pd.DataFrame(brands_data)


# --- Streamlit ArayÃ¼zÃ¼ ---
url = st.text_input("Sephora ÃœrÃ¼n Listeleme SayfasÄ± URL'sini Girin:", placeholder="https://www.sephora.me/ae-en/shop/fragrance/C301")
process_button = st.button("MarkalarÄ± Ã‡ek ve CSV OluÅŸtur")

if process_button and url:
    if not url.startswith(('http://', 'https://')) or "sephora." not in url:
         st.warning("LÃ¼tfen geÃ§erli bir Sephora URL'si girin (http:// veya https:// ile baÅŸlamalÄ±dÄ±r).")
    else:
        with st.spinner("Sayfa indiriliyor (requests) ve markalar Ã§ekiliyor... LÃ¼tfen bekleyin."):
            # Ã–nce requests ile dene
            html_content = fetch_html_requests(url)

            if html_content:
                df_brands = extract_brands_from_html(html_content) # HTML'i iÅŸle

                if df_brands is not None and not df_brands.empty:
                    st.subheader("Ã‡ekilen Marka Verileri")
                    st.dataframe(df_brands, use_container_width=True)

                    try:
                        csv_buffer = StringIO()
                        df_brands.to_csv(csv_buffer, index=False, encoding='utf-8')
                        csv_data = csv_buffer.getvalue()
                        try:
                            path_part = url.split('/')[-1].split('?')[0]
                            safe_part = re.sub(r'[\\/*?:"<>|]', "-", path_part)
                            safe_part = safe_part[:50] if len(safe_part) > 50 else safe_part
                            csv_filename = f"sephora_markalar_{safe_part}.csv" if safe_part else "sephora_markalar.csv"
                        except Exception:
                            csv_filename = "sephora_markalar.csv"

                        st.download_button(
                            label="ğŸ’¾ CSV Olarak Ä°ndir",
                            data=csv_data,
                            file_name=csv_filename,
                            mime='text/csv',
                        )
                    except Exception as e:
                        st.error(f"CSV dosyasÄ± oluÅŸturulurken veya indirme butonu hazÄ±rlanÄ±rken hata: {e}")

                elif df_brands is not None and df_brands.empty:
                     st.warning("Marka verisi bulunamadÄ±. URL'yi, filtrelerin gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ kontrol edin veya sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir.")
            # else: fetch_html_requests iÃ§inde hata mesajÄ± zaten verildi.

elif process_button and not url:
    st.warning("LÃ¼tfen bir URL girin.")

st.markdown("---")
st.caption("Not: Bu uygulama Sephora web sitesinin yapÄ±sÄ±na baÄŸlÄ±dÄ±r. Site gÃ¼ncellenirse veya gÃ¼venlik Ã¶nlemleri (WAF vb.) kullanÄ±lÄ±rsa `requests` ile veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z olabilir.")
